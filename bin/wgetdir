#!/usr/bin/env python
"""
usage:
    %prog url

Description:
    Recursively download all the contents of the remote directory to the
    current directory.

    Autogenerated index files are removed from the cwd, as is robots.txt. but
    not removed recursively.  That's a TODO

"""

import os
import sys
import subprocess
import glob

from optparse import OptionParser

parser=OptionParser(__doc__)
parser.add_option("-r","--recurse",action="store_true",
                  help="copy files recursively the directory")

junk="""
parser.add_option("--noindex",action="store_true",
                  help="Don't download an index.html file.  wgetdirs "
                  "will always skip the index.html?... files but usually "
                  "will allow normal index.html since that may in fact "
                  "be a user file.  This disables that behaviour.")
"""

def cleanup():
    """
    Remove index.html?* files.  Note this is *not* yet recursive.
    """

    # remove files like index.html?
    badf = glob.glob('index.html?*')
    for f in badf:
        os.remove(f)
    if len(badf) != 0:
        # if we found index.html? files, then the
        # any file called index.html was auto-generated
        # and we want to remove it too
        if os.path.exists('index.html'):
            os.remove('index.html')
    if os.path.exists('robots.txt'):
        os.remove('robots.txt')

def get_cut_dirs(url):

    url = url.strip()
    if url[0:3] == 'ftp':
        remote_dir = url[6:]
    elif url[0:4] == 'http':
        remote_dir = url[7:]
    else:
        raise ValueError("url does not contain 'http' or 'ftp'\n    '%s'" % url)

    if remote_dir[-1] == '/':
        remote_dir = remote_dir[0:-1]

    # the remaining number of / will give the cut-dirs
    cut_dirs = remote_dir.count('/')
    return cut_dirs

def main():
    options, args = parser.parse_args(sys.argv[1:])

    if len(args) < 1:
        parser.print_help()
        sys.exit(1)

    url = args[0]

    recurse=options.recurse
    if recurse:
        depth='inf'
    else:
        depth='1'
    cut_dirs = get_cut_dirs(url)

    
    command=['wget',
             '-N',
             '-r',
             '-l '+depth,
             '-nH',  # -nH prevents putting hostname directory here
             '--cut-dirs=%s' % cut_dirs,
             '--no-parent', # never ascend to parent dir
             '--tries=50',
             "--reject='index.html*'",
             url]

    print command
    retcode = subprocess.call(command)
    cleanup()

if __name__=="__main__":
    main()
