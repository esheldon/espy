#!/usr/bin/env python
"""
usage:
    uget url

purpose:
    Download the url to the current directory.  Can be either a local file
    or a internet url. Uses the python urllib library.
"""

import sys
from sys import stdout
import os

def expand_path(path):
    path = os.path.expandvars(path)
    path = os.path.expanduser(path)
    return path

def is_local(url):
    return os.path.exists(expand_path(url))

def get_web_basename(url):
    # separator on the web is always /
    urlsplit=url.split('/')
    if urlsplit[-1] != '':
        name=urlsplit[-1]
    else:
        name=urlsplit[-2]
    return name

def get_urllib(url, dest):
    import urllib
    urllib.urlretrieve(url, dest)

def get_urllib2(url, dest):
    import urllib2
    req = urllib2.Request(url)
    f = urllib2.urlopen(req)
    local_file = open(dest, 'w')
    local_file.write( f.read() )
    local_file.close()

def get_urllib2_proxy(url, dest):
    import urllib2
    opener = urllib2.build_opener(urllib2.ProxyBasicAuthHandler)
    f = opener.open(url)
    local_file = open(dest, 'w')
    local_file.write( f.read() )
    local_file.close()


def main():
    if len(sys.argv) < 2:
        stdout.write(__doc__)
        sys.exit(45)
    
    url = sys.argv[1]
    if is_local(url):
        # It's a local file
        url = expand_path(url)
        dest = os.path.basename(url)
    else:
        # Its a web/ftp address
        dest = get_web_basename(url)

    stdout.write("\nCopying from '%s' to '%s'\n" % (url, dest))
    #get_urllib2(url, dest)
    try:
        get_urllib2(url, dest)
    except:
        get_urllib2_proxy(url, dest)
        # except:
        #    raise RuntimeError("Could not download URL: '%s'" % url)


if __name__ == '__main__':
    main()
